#!/usr/bin/env python3
"""
é«˜çº§ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºLLM4TestGençš„é«˜çº§åŠŸèƒ½å’Œè‡ªå®šä¹‰ç”¨æ³•
"""

import sys
from pathlib import Path
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

def example_custom_vector_store():
    """ç¤ºä¾‹1: è‡ªå®šä¹‰å‘é‡å­˜å‚¨"""
    print("ç¤ºä¾‹1: è‡ªå®šä¹‰å‘é‡å­˜å‚¨é…ç½®")
    print("=" * 50)
    
    from rag.vector_store import CodeVectorStore
    from rag.project_analyzer import SmartProjectAnalyzer
    
    # åˆ›å»ºè‡ªå®šä¹‰å‘é‡å­˜å‚¨
    custom_vector_store = CodeVectorStore(
        collection_name="my_custom_project",
        persist_directory="./custom_chroma_db"
    )
    
    # æ‰‹åŠ¨æ·»åŠ ä»£ç ç‰‡æ®µ
    code_snippets = [
        {
            'code': '''
            public class CustomCalculator {
                public double power(double base, double exponent) {
                    return Math.pow(base, exponent);
                }
            }
            ''',
            'metadata': {
                'type': 'class',
                'class_name': 'CustomCalculator',
                'file_path': 'custom/Calculator.java',
                'language': 'java'
            }
        },
        {
            'code': '''
            public double sqrt(double number) {
                if (number < 0) {
                    throw new IllegalArgumentException("Cannot compute square root of negative number");
                }
                return Math.sqrt(number);
            }
            ''',
            'metadata': {
                'type': 'method',
                'method_name': 'sqrt',
                'class_name': 'MathUtils',
                'language': 'java'
            }
        }
    ]
    
    # æ‰¹é‡æ·»åŠ 
    codes = [snippet['code'] for snippet in code_snippets]
    metadatas = [snippet['metadata'] for snippet in code_snippets]
    
    doc_ids = custom_vector_store.add_batch_code_snippets(codes, metadatas)
    print(f" æ·»åŠ äº† {len(doc_ids)} ä¸ªä»£ç ç‰‡æ®µ")
    
    # æœç´¢æµ‹è¯•
    results = custom_vector_store.search_similar_code(
        query="mathematical calculation power function",
        top_k=3
    )
    
    print(f" æœç´¢ç»“æœ: {len(results)} ä¸ªåŒ¹é…é¡¹")
    for i, result in enumerate(results, 1):
        metadata = result['metadata']
        similarity = 1 - result.get('distance', 1.0)
        print(f"   {i}. {metadata.get('class_name', 'Unknown')} (ç›¸ä¼¼åº¦: {similarity:.3f})")
    
    return len(results) > 0

def example_parallel_generation():
    """ç¤ºä¾‹2: å¹¶è¡Œæµ‹è¯•ç”Ÿæˆ"""
    print("\nç¤ºä¾‹2: å¹¶è¡Œæµ‹è¯•ç”Ÿæˆ")
    print("=" * 50)
    
    from enhanced_test_generator import EnhancedTestGenerator
    
    # è¦æµ‹è¯•çš„æ–¹æ³•åˆ—è¡¨
    methods_to_test = [
        ("com.example.Calculator", "add"),
        ("com.example.Calculator", "subtract"), 
        ("com.example.Calculator", "multiply"),
        ("com.example.MathUtils", "factorial"),
        ("com.example.StringUtils", "reverse"),
    ]
    
    project_path = Path("./sample-java-project")
    generator = EnhancedTestGenerator(project_path)
    
    def generate_single_test(class_name, method_name):
        """ç”Ÿæˆå•ä¸ªæµ‹è¯•çš„å‡½æ•°"""
        start_time = time.time()
        result = generator.generate_test_for_method(
            class_name=class_name,
            method_name=method_name,
            use_rag=True
        )
        end_time = time.time()
        
        return {
            'class_name': class_name,
            'method_name': method_name,
            'result': result,
            'duration': end_time - start_time
        }
    
    print(f" å¼€å§‹å¹¶è¡Œç”Ÿæˆ {len(methods_to_test)} ä¸ªæµ‹è¯•...")
    
    # å¹¶è¡Œæ‰§è¡Œ
    with ThreadPoolExecutor(max_workers=3) as executor:
        # æäº¤ä»»åŠ¡
        future_to_method = {
            executor.submit(generate_single_test, class_name, method_name): (class_name, method_name)
            for class_name, method_name in methods_to_test
        }
        
        results = []
        completed = 0
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_method):
            completed += 1
            class_name, method_name = future_to_method[future]
            
            try:
                result_data = future.result()
                results.append(result_data)
                
                success = result_data['result']['success']
                duration = result_data['duration']
                status = "" if success else ""
                
                print(f"   [{completed}/{len(methods_to_test)}] {status} {class_name}.{method_name} ({duration:.1f}s)")
                
            except Exception as e:
                print(f"   [{completed}/{len(methods_to_test)}]  {class_name}.{method_name} å¼‚å¸¸: {e}")
    
    # ç»Ÿè®¡ç»“æœ
    successful = sum(1 for r in results if r['result']['success'])
    total_time = sum(r['duration'] for r in results)
    avg_time = total_time / len(results) if results else 0
    
    print(f"\n å¹¶è¡Œç”Ÿæˆç»Ÿè®¡:")
    print(f"   æˆåŠŸ: {successful}/{len(results)}")
    print(f"   æ€»è€—æ—¶: {total_time:.1f}s")
    print(f"   å¹³å‡è€—æ—¶: {avg_time:.1f}s")
    print(f"   æˆåŠŸç‡: {successful/len(results):.1%}")
    
    return successful > 0

def example_custom_prompts():
    """ç¤ºä¾‹3: è‡ªå®šä¹‰æç¤ºå·¥ç¨‹"""
    print("\nç¤ºä¾‹3: è‡ªå®šä¹‰æç¤ºå·¥ç¨‹")
    print("=" * 50)
    
    def create_advanced_test_prompt(method_info, context_info, test_style="comprehensive"):
        """åˆ›å»ºé«˜çº§æµ‹è¯•æç¤º"""
        
        base_prompt = f"""Generate JUnit 5 test methods for: {method_info['signature']}

Test Style: {test_style.upper()}
"""
        
        if test_style == "comprehensive":
            base_prompt += """
Requirements:
1. Test normal/happy path scenarios
2. Test edge cases and boundary conditions  
3. Test error conditions with proper exception handling
4. Use parameterized tests where appropriate
5. Include performance considerations if relevant
6. Add detailed documentation for each test case
"""
        elif test_style == "minimal":
            base_prompt += """
Requirements:
1. Test basic functionality only
2. Keep tests simple and focused
3. Minimal assertions
"""
        elif test_style == "bdd":
            base_prompt += """
Requirements:
1. Use BDD-style test naming (given_when_then)
2. Structure tests with Given/When/Then comments
3. Focus on behavior specification
4. Use descriptive test method names
"""
        
        # æ·»åŠ ä¸Šä¸‹æ–‡
        if context_info:
            base_prompt += f"\nRelevant Context from Codebase:\n"
            for i, context in enumerate(context_info[:3], 1):
                metadata = context.get('metadata', {})
                code_snippet = context.get('code', '')[:150]
                base_prompt += f"\n{i}. Related: {metadata.get('class_name', 'Unknown')}.{metadata.get('method_name', 'Unknown')}\n"
                base_prompt += f"   Code: {code_snippet}...\n"
        
        base_prompt += "\nGenerate the complete test class with proper imports:"
        
        return base_prompt
    
    # ç¤ºä¾‹æ–¹æ³•ä¿¡æ¯
    method_info = {
        'signature': 'public int divide(int dividend, int divisor)',
        'class_name': 'Calculator',
        'method_name': 'divide'
    }
    
    # æ¨¡æ‹Ÿä¸Šä¸‹æ–‡ä¿¡æ¯
    mock_context = [
        {
            'metadata': {'class_name': 'Calculator', 'method_name': 'multiply'},
            'code': 'public int multiply(int a, int b) { return a * b; }'
        }
    ]
    
    # ä¸åŒé£æ ¼çš„æç¤º
    styles = ["comprehensive", "minimal", "bdd"]
    
    for style in styles:
        print(f"\n {style.upper()} é£æ ¼æç¤º:")
        prompt = create_advanced_test_prompt(method_info, mock_context, style)
        print(f"   æç¤ºé•¿åº¦: {len(prompt)} å­—ç¬¦")
        print(f"   å‰100å­—ç¬¦: {prompt[:100]}...")
    
    return True

def example_quality_metrics():
    """ç¤ºä¾‹4: æµ‹è¯•è´¨é‡è¯„ä¼°"""
    print("\nç¤ºä¾‹4: æµ‹è¯•è´¨é‡è¯„ä¼°")
    print("=" * 50)
    
    def analyze_test_quality(generated_test_code):
        """åˆ†æç”Ÿæˆæµ‹è¯•çš„è´¨é‡"""
        quality_metrics = {
            'has_imports': False,
            'has_test_annotation': False,
            'has_assertions': False,
            'has_exception_tests': False,
            'has_setup_teardown': False,
            'test_method_count': 0,
            'assertion_count': 0,
            'line_count': 0
        }
        
        lines = generated_test_code.split('\n')
        quality_metrics['line_count'] = len(lines)
        
        for line in lines:
            line = line.strip()
            
            # æ£€æŸ¥å¯¼å…¥
            if line.startswith('import'):
                quality_metrics['has_imports'] = True
            
            # æ£€æŸ¥æµ‹è¯•æ³¨è§£
            if '@Test' in line:
                quality_metrics['has_test_annotation'] = True
                quality_metrics['test_method_count'] += 1
            
            # æ£€æŸ¥æ–­è¨€
            if 'assert' in line.lower():
                quality_metrics['has_assertions'] = True
                quality_metrics['assertion_count'] += 1
            
            # æ£€æŸ¥å¼‚å¸¸æµ‹è¯•
            if 'assertThrows' in line or 'expectedException' in line:
                quality_metrics['has_exception_tests'] = True
            
            # æ£€æŸ¥Setup/Teardown
            if '@BeforeEach' in line or '@AfterEach' in line:
                quality_metrics['has_setup_teardown'] = True
        
        return quality_metrics
    
    def calculate_quality_score(metrics):
        """è®¡ç®—è´¨é‡åˆ†æ•° (0-100)"""
        score = 0
        
        # åŸºç¡€ç»“æ„ (40åˆ†)
        if metrics['has_imports']: score += 10
        if metrics['has_test_annotation']: score += 15
        if metrics['has_assertions']: score += 15
        
        # æµ‹è¯•å®Œæ•´æ€§ (40åˆ†)
        score += min(metrics['test_method_count'] * 5, 20)  # æœ€å¤š20åˆ†
        score += min(metrics['assertion_count'] * 2, 20)    # æœ€å¤š20åˆ†
        
        # é«˜çº§ç‰¹æ€§ (20åˆ†)
        if metrics['has_exception_tests']: score += 10
        if metrics['has_setup_teardown']: score += 10
        
        return min(score, 100)
    
    # ç¤ºä¾‹æµ‹è¯•ä»£ç 
    sample_test_code = '''
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.BeforeEach;
import static org.junit.jupiter.api.Assertions.*;

public class CalculatorTest {
    
    private Calculator calculator;
    
    @BeforeEach
    void setUp() {
        calculator = new Calculator();
    }
    
    @Test
    void testAdd() {
        assertEquals(5, calculator.add(2, 3));
        assertEquals(0, calculator.add(0, 0));
        assertEquals(-1, calculator.add(-3, 2));
    }
    
    @Test
    void testDivideByZero() {
        assertThrows(IllegalArgumentException.class, 
            () -> calculator.divide(10, 0));
    }
}
'''
    
    # åˆ†æè´¨é‡
    metrics = analyze_test_quality(sample_test_code)
    score = calculate_quality_score(metrics)
    
    print(f" æµ‹è¯•è´¨é‡åˆ†æ:")
    print(f"   ä»£ç è¡Œæ•°: {metrics['line_count']}")
    print(f"   æµ‹è¯•æ–¹æ³•æ•°: {metrics['test_method_count']}")
    print(f"   æ–­è¨€æ•°é‡: {metrics['assertion_count']}")
    print(f"   åŒ…å«å¯¼å…¥: {'' if metrics['has_imports'] else ''}")
    print(f"   åŒ…å«@Testæ³¨è§£: {'' if metrics['has_test_annotation'] else ''}")
    print(f"   åŒ…å«æ–­è¨€: {'' if metrics['has_assertions'] else ''}")
    print(f"   åŒ…å«å¼‚å¸¸æµ‹è¯•: {'' if metrics['has_exception_tests'] else ''}")
    print(f"   åŒ…å«Setup/Teardown: {'' if metrics['has_setup_teardown'] else ''}")
    
    print(f"\nğŸ† è´¨é‡åˆ†æ•°: {score}/100")
    
    if score >= 80:
        print("   è¯„çº§: ä¼˜ç§€ â­â­â­")
    elif score >= 60:
        print("   è¯„çº§: è‰¯å¥½ â­â­")
    elif score >= 40:
        print("   è¯„çº§: ä¸€èˆ¬ â­")
    else:
        print("   è¯„çº§: éœ€è¦æ”¹è¿›")
    
    return score >= 60

def example_custom_filters():
    """ç¤ºä¾‹5: è‡ªå®šä¹‰ä»£ç è¿‡æ»¤"""
    print("\nç¤ºä¾‹5: è‡ªå®šä¹‰ä»£ç è¿‡æ»¤å’Œæ£€ç´¢")
    print("=" * 50)
    
    from rag.vector_store import CodeVectorStore
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = CodeVectorStore("filtered_search_demo")
    
    # æ·»åŠ å¤šæ ·åŒ–çš„ä»£ç ç¤ºä¾‹
    diverse_code_samples = [
        {
            'code': 'public void setUp() { /* initialization */ }',
            'metadata': {
                'type': 'method',
                'method_name': 'setUp',
                'access_modifier': 'public',
                'category': 'test_setup',
                'complexity': 'simple',
                'language': 'java'
            }
        },
        {
            'code': 'private boolean validateInput(String input) { return input != null && !input.isEmpty(); }',
            'metadata': {
                'type': 'method', 
                'method_name': 'validateInput',
                'access_modifier': 'private',
                'category': 'validation',
                'complexity': 'simple',
                'language': 'java'
            }
        },
        {
            'code': 'public List<Integer> complexAlgorithm(int[] data) { /* complex logic */ }',
            'metadata': {
                'type': 'method',
                'method_name': 'complexAlgorithm', 
                'access_modifier': 'public',
                'category': 'algorithm',
                'complexity': 'complex',
                'language': 'java'
            }
        }
    ]
    
    # æ·»åŠ æ ·æœ¬
    for sample in diverse_code_samples:
        vector_store.add_code_snippet(sample['code'], sample['metadata'])
    
    print(f" æ·»åŠ äº† {len(diverse_code_samples)} ä¸ªä»£ç æ ·æœ¬")
    
    # ä¸åŒçš„è¿‡æ»¤æŸ¥è¯¢
    filter_queries = [
        {
            'name': 'åªæŸ¥æ‰¾publicæ–¹æ³•',
            'query': 'method implementation',
            'filter': {'access_modifier': 'public'}
        },
        {
            'name': 'åªæŸ¥æ‰¾éªŒè¯ç›¸å…³æ–¹æ³•',
            'query': 'validation logic',
            'filter': {'category': 'validation'}
        },
        {
            'name': 'åªæŸ¥æ‰¾ç®€å•æ–¹æ³•',
            'query': 'simple function',
            'filter': {'complexity': 'simple'}
        },
        {
            'name': 'å¤åˆè¿‡æ»¤ï¼špublic + simple',
            'query': 'method',
            'filter': {'access_modifier': 'public', 'complexity': 'simple'}
        }
    ]
    
    # æ‰§è¡Œè¿‡æ»¤æŸ¥è¯¢
    for query_info in filter_queries:
        print(f"\n {query_info['name']}:")
        
        results = vector_store.search_similar_code(
            query=query_info['query'],
            top_k=5,
            filter_metadata=query_info['filter']
        )
        
        if results:
            print(f"   æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:")
            for i, result in enumerate(results, 1):
                metadata = result['metadata']
                method_name = metadata.get('method_name', 'unknown')
                access_mod = metadata.get('access_modifier', 'unknown')
                category = metadata.get('category', 'unknown')
                print(f"      {i}. {access_mod} {method_name} ({category})")
        else:
            print("   æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç»“æœ")
    
    return True

def example_monitoring_and_logging():
    """ç¤ºä¾‹6: ç›‘æ§å’Œæ—¥å¿—"""
    print("\nç¤ºä¾‹6: ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ")
    print("=" * 50)
    
    import logging
    from datetime import datetime
    
    # è®¾ç½®è¯¦ç»†æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('llm4testgen.log'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger('LLM4TestGen')
    
    class GenerationMonitor:
        """ç”Ÿæˆè¿‡ç¨‹ç›‘æ§å™¨"""
        
        def __init__(self):
            self.stats = {
                'total_attempts': 0,
                'successful_generations': 0,
                'failed_generations': 0,
                'total_time': 0,
                'context_retrievals': 0,
                'avg_context_size': 0
            }
            self.start_time = None
        
        def start_generation(self, class_name, method_name):
            """å¼€å§‹ç”Ÿæˆç›‘æ§"""
            self.start_time = time.time()
            self.stats['total_attempts'] += 1
            logger.info(f"å¼€å§‹ç”Ÿæˆæµ‹è¯•: {class_name}.{method_name}")
        
        def end_generation(self, success, context_count=0):
            """ç»“æŸç”Ÿæˆç›‘æ§"""
            if self.start_time:
                duration = time.time() - self.start_time
                self.stats['total_time'] += duration
                
                if success:
                    self.stats['successful_generations'] += 1
                    logger.info(f"ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶ {duration:.2f}sï¼Œä½¿ç”¨ {context_count} ä¸ªä¸Šä¸‹æ–‡")
                else:
                    self.stats['failed_generations'] += 1
                    logger.warning(f"ç”Ÿæˆå¤±è´¥ï¼Œè€—æ—¶ {duration:.2f}s")
                
                if context_count > 0:
                    self.stats['context_retrievals'] += 1
                    # æ›´æ–°å¹³å‡ä¸Šä¸‹æ–‡å¤§å°
                    current_avg = self.stats['avg_context_size']
                    retrievals = self.stats['context_retrievals']
                    self.stats['avg_context_size'] = (current_avg * (retrievals - 1) + context_count) / retrievals
        
        def get_summary(self):
            """è·å–ç›‘æ§æ‘˜è¦"""
            if self.stats['total_attempts'] > 0:
                success_rate = self.stats['successful_generations'] / self.stats['total_attempts']
                avg_time = self.stats['total_time'] / self.stats['total_attempts']
            else:
                success_rate = 0
                avg_time = 0
            
            return {
                'success_rate': success_rate,
                'avg_generation_time': avg_time,
                'total_attempts': self.stats['total_attempts'],
                'avg_context_size': self.stats['avg_context_size']
            }
    
    # ä½¿ç”¨ç›‘æ§å™¨
    monitor = GenerationMonitor()
    
    # æ¨¡æ‹Ÿå‡ æ¬¡ç”Ÿæˆ
    test_methods = [
        ("Calculator", "add", True, 3),
        ("Calculator", "divide", False, 2),
        ("StringUtils", "reverse", True, 1),
    ]
    
    for class_name, method_name, success, context_count in test_methods:
        monitor.start_generation(class_name, method_name)
        time.sleep(0.1)  # æ¨¡æ‹Ÿç”Ÿæˆæ—¶é—´
        monitor.end_generation(success, context_count)
    
    # ç”ŸæˆæŠ¥å‘Š
    summary = monitor.get_summary()
    
    print(f" ç”Ÿæˆç›‘æ§æŠ¥å‘Š:")
    print(f"   æ€»å°è¯•æ¬¡æ•°: {summary['total_attempts']}")
    print(f"   æˆåŠŸç‡: {summary['success_rate']:.1%}")
    print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {summary['avg_generation_time']:.3f}s")
    print(f"   å¹³å‡ä¸Šä¸‹æ–‡å¤§å°: {summary['avg_context_size']:.1f}")
    
    # æ—¥å¿—æ–‡ä»¶ä¿¡æ¯
    log_file = Path('llm4testgen.log')
    if log_file.exists():
        print(f"\n æ—¥å¿—æ–‡ä»¶: {log_file}")
        print(f"   æ–‡ä»¶å¤§å°: {log_file.stat().st_size} å­—èŠ‚")
    
    return True

def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰é«˜çº§ç¤ºä¾‹"""
    print(" LLM4TestGen é«˜çº§ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 70)
    
    examples = [
        ("è‡ªå®šä¹‰å‘é‡å­˜å‚¨", example_custom_vector_store),
        ("å¹¶è¡Œæµ‹è¯•ç”Ÿæˆ", example_parallel_generation),
        ("è‡ªå®šä¹‰æç¤ºå·¥ç¨‹", example_custom_prompts),
        ("æµ‹è¯•è´¨é‡è¯„ä¼°", example_quality_metrics),
        ("è‡ªå®šä¹‰ä»£ç è¿‡æ»¤", example_custom_filters),
        ("ç›‘æ§å’Œæ—¥å¿—", example_monitoring_and_logging),
    ]
    
    results = []
    
    for name, func in examples:
        try:
            print(f"\n è¿è¡Œé«˜çº§ç¤ºä¾‹: {name}")
            success = func()
            results.append((name, success))
            
            if success:
                print(f" {name} ç¤ºä¾‹å®Œæˆ")
            else:
                print(f" {name} ç¤ºä¾‹éƒ¨åˆ†æˆåŠŸ")
                
        except Exception as e:
            print(f" {name} ç¤ºä¾‹å¼‚å¸¸: {e}")
            results.append((name, False))
    
    # æ€»ç»“
    print(f"\n{'=' * 70}")
    print(" é«˜çº§ç¤ºä¾‹è¿è¡Œæ€»ç»“:")
    print("=" * 70)
    
    successful = sum(1 for _, success in results if success)
    total = len(results)
    
    for name, success in results:
        status = "" if success else ""
        print(f"   {status} {name}")
    
    print(f"\n æˆåŠŸç‡: {successful}/{total} ({successful/total:.1%})")
    
    print(f"\n é«˜çº§åŠŸèƒ½ç‰¹ç‚¹:")
    print("    å¹¶è¡Œç”Ÿæˆæå‡æ•ˆç‡")
    print("    è‡ªå®šä¹‰æç¤ºå·¥ç¨‹")
    print("    è´¨é‡è¯„ä¼°å’Œç›‘æ§")
    print("    æ™ºèƒ½è¿‡æ»¤å’Œæ£€ç´¢")
    print("    è¯¦ç»†æ—¥å¿—è®°å½•")
    
    print(f"\n è¿›é˜¶ä½¿ç”¨å»ºè®®:")
    print("   1. æ ¹æ®é¡¹ç›®éœ€æ±‚è‡ªå®šä¹‰é…ç½®")
    print("   2. ä½¿ç”¨å¹¶è¡Œç”Ÿæˆæå‡å¤§é¡¹ç›®æ•ˆç‡")
    print("   3. å»ºç«‹è´¨é‡è¯„ä¼°æ ‡å‡†")
    print("   4. ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")

if __name__ == "__main__":
    main()
