#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„è¿œç¨‹Ollamaé…ç½®è„šæœ¬
å¤„ç†æ¨¡å‹åŠ è½½æ…¢å’Œè¶…æ—¶é—®é¢˜
"""

import sys
import requests
import json
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from config.remote_ollama_config import remote_config

def test_connection_with_patience(base_url, timeout_seconds=15):
    """è€å¿ƒåœ°æµ‹è¯•è¿æ¥"""
    print(f"ğŸŒ æµ‹è¯•è¿æ¥åˆ°: {base_url}")
    
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=timeout_seconds)
        if response.status_code == 200:
            models_data = response.json()
            models = [model.get('name', '') for model in models_data.get('models', [])]
            print(f" è¿æ¥æˆåŠŸï¼Œæ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹")
            return models
        else:
            print(f" è¿æ¥å¤±è´¥: HTTP {response.status_code}")
            return None
    except Exception as e:
        print(f" è¿æ¥å¤±è´¥: {e}")
        return None

def warm_up_model(base_url, model_name, max_wait_time=120):
    """é¢„çƒ­æ¨¡å‹ï¼ˆé¿å…é¦–æ¬¡ä½¿ç”¨è¶…æ—¶ï¼‰"""
    print(f" é¢„çƒ­æ¨¡å‹: {model_name}")
    print("   è¿™å¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    # ä½¿ç”¨ç®€å•çš„generate APIè¿›è¡Œé¢„çƒ­
    warm_up_request = {
        "model": model_name,
        "prompt": "hi",
        "stream": False,
        "options": {
            "num_predict": 5  # åªç”Ÿæˆ5ä¸ªtokenï¼Œå¿«é€Ÿé¢„çƒ­
        }
    }
    
    try:
        start_time = time.time()
        response = requests.post(
            f"{base_url}/api/generate",
            json=warm_up_request,
            timeout=max_wait_time
        )
        
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            print(f"    æ¨¡å‹é¢„çƒ­æˆåŠŸ ({elapsed:.1f}ç§’)")
            return True
        else:
            print(f"    æ¨¡å‹é¢„çƒ­å¤±è´¥: {response.status_code}")
            print(f"   é”™è¯¯ä¿¡æ¯: {response.text}")
            return False
            
    except requests.exceptions.Timeout:
        print(f"   â° æ¨¡å‹é¢„çƒ­è¶…æ—¶ (>{max_wait_time}ç§’)")
        print(f"    æ¨¡å‹å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´åŠ è½½ï¼Œå»ºè®®åœ¨è¿œç¨‹æœåŠ¡å™¨ä¸Šæ‰‹åŠ¨é¢„çƒ­:")
        print(f"       ollama run {model_name}")
        return False
    except Exception as e:
        print(f"    é¢„çƒ­å¼‚å¸¸: {e}")
        return False

def test_embedding_with_retry(base_url, model_name, max_retries=3):
    """å¸¦é‡è¯•çš„åµŒå…¥æµ‹è¯•"""
    print(f"ğŸ”¤ æµ‹è¯•åµŒå…¥æ¨¡å‹: {model_name}")
    
    for attempt in range(1, max_retries + 1):
        print(f"   å°è¯• {attempt}/{max_retries}...")
        
        try:
            embed_request = {
                "model": model_name,
                "prompt": "test"
            }
            
            response = requests.post(
                f"{base_url}/api/embeddings",
                json=embed_request,
                timeout=60
            )
            
            if response.status_code == 200:
                embed_data = response.json()
                if 'embedding' in embed_data:
                    embedding = embed_data['embedding']
                    print(f"    åµŒå…¥æµ‹è¯•æˆåŠŸ: {len(embedding)} ç»´å‘é‡")
                    return True
                else:
                    print(f"    åµŒå…¥å“åº”æ ¼å¼é”™è¯¯")
                    return False
            else:
                print(f"    åµŒå…¥è¯·æ±‚å¤±è´¥: {response.status_code}")
                if attempt < max_retries:
                    print(f"   ç­‰å¾…3ç§’åé‡è¯•...")
                    time.sleep(3)
                else:
                    print(f"   é”™è¯¯ä¿¡æ¯: {response.text}")
                    return False
                
        except requests.exceptions.Timeout:
            print(f"   â° è¯·æ±‚è¶…æ—¶")
            if attempt < max_retries:
                print(f"   ç­‰å¾…5ç§’åé‡è¯•...")
                time.sleep(5)
            else:
                return False
        except Exception as e:
            print(f"    è¯·æ±‚å¼‚å¸¸: {e}")
            if attempt < max_retries:
                print(f"   ç­‰å¾…3ç§’åé‡è¯•...")
                time.sleep(3)
            else:
                return False
    
    return False

def test_chat_lightweight(base_url, model_name):
    """è½»é‡çº§èŠå¤©æµ‹è¯•"""
    print(f" æµ‹è¯•ä»£ç ç”Ÿæˆæ¨¡å‹: {model_name}")
    
    # ä½¿ç”¨æ›´çŸ­çš„æç¤ºå’Œæ›´å°‘çš„è¾“å‡º
    lightweight_request = {
        "model": model_name,
        "prompt": "å†™ä¸€ä¸ªaddæ–¹æ³•",
        "stream": False,
        "options": {
            "num_predict": 50,  # é™åˆ¶è¾“å‡ºé•¿åº¦
            "temperature": 0.1  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
        }
    }
    
    try:
        print("   å‘é€è½»é‡çº§æµ‹è¯•è¯·æ±‚...")
        response = requests.post(
            f"{base_url}/api/generate",
            json=lightweight_request,
            timeout=90
        )
        
        if response.status_code == 200:
            data = response.json()
            if 'response' in data:
                content = data['response']
                print(f"    ä»£ç ç”Ÿæˆæµ‹è¯•æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
                if any(keyword in content.lower() for keyword in ['public', 'int', 'add', 'return']):
                    print(f"    åŒ…å«é¢„æœŸçš„Javaä»£ç å…³é”®è¯")
                return True
            else:
                print(f"    å“åº”æ ¼å¼é”™è¯¯")
                return False
        else:
            print(f"    è¯·æ±‚å¤±è´¥: {response.status_code}")
            print(f"   é”™è¯¯ä¿¡æ¯: {response.text}")
            return False
            
    except requests.exceptions.Timeout:
        print(f"   â° è¯·æ±‚è¶…æ—¶ï¼Œæ¨¡å‹å“åº”å¤ªæ…¢")
        print(f"    å»ºè®®: åœ¨è¿œç¨‹æœåŠ¡å™¨æ£€æŸ¥æ¨¡å‹çŠ¶æ€")
        return False
    except Exception as e:
        print(f"    è¯·æ±‚å¼‚å¸¸: {e}")
        return False

def setup_optimized_remote():
    """è®¾ç½®ä¼˜åŒ–çš„è¿œç¨‹é…ç½®"""
    print(" ä¼˜åŒ–çš„è¿œç¨‹Ollamaé…ç½®")
    print("="*60)
    
    # ä»ä¹‹å‰çš„é…ç½®ä¸­è¯»å–ä¿¡æ¯ï¼Œæˆ–é‡æ–°è¾“å…¥
    print("æ£€æµ‹åˆ°æ‚¨çš„æ¨¡å‹é…ç½®:")
    print("ğŸ¤– ä»£ç ç”Ÿæˆ: qwen3-8b-q4:latest")
    print("ğŸ”¤ åµŒå…¥æ¨¡å‹: qwen3-embedding:latest")
    print("ğŸŒ æœåŠ¡å™¨: http://localhost:11434")
    
    confirm = input("\nä½¿ç”¨è¿™ä¸ªé…ç½®ç»§ç»­å—? (y/n, é»˜è®¤y): ").strip().lower()
    if confirm == 'n':
        print("è¯·é‡æ–°è¿è¡Œ setup_remote_only.py è¿›è¡Œé…ç½®")
        return False
    
    base_url = "http://localhost:11434"
    code_model = "qwen3-8b-q4:latest"
    embed_model = "qwen3-embedding:latest"
    
    # 1. æµ‹è¯•åŸºæœ¬è¿æ¥
    models = test_connection_with_patience(base_url)
    if not models:
        return False
    
    # 2. é¢„çƒ­åµŒå…¥æ¨¡å‹ï¼ˆé€šå¸¸æ›´å¿«ï¼‰
    print(f"\n é¢„çƒ­åµŒå…¥æ¨¡å‹...")
    if warm_up_model(base_url, embed_model, max_wait_time=60):
        # 3. æµ‹è¯•åµŒå…¥åŠŸèƒ½
        embed_success = test_embedding_with_retry(base_url, embed_model)
    else:
        print("    åµŒå…¥æ¨¡å‹é¢„çƒ­å¤±è´¥ï¼Œè·³è¿‡åµŒå…¥æµ‹è¯•")
        embed_success = False
    
    # 4. é¢„çƒ­ä»£ç ç”Ÿæˆæ¨¡å‹
    print(f"\n é¢„çƒ­ä»£ç ç”Ÿæˆæ¨¡å‹...")
    if warm_up_model(base_url, code_model, max_wait_time=120):
        # 5. æµ‹è¯•ä»£ç ç”ŸæˆåŠŸèƒ½
        chat_success = test_chat_lightweight(base_url, code_model)
    else:
        print("    ä»£ç æ¨¡å‹é¢„çƒ­å¤±è´¥ï¼Œè·³è¿‡èŠå¤©æµ‹è¯•")
        chat_success = False
    
    # 6. è¯„ä¼°ç»“æœ
    if embed_success and chat_success:
        status = "å®Œå…¨æ­£å¸¸"
        config_type = "full"
    elif embed_success:
        status = "åµŒå…¥æ­£å¸¸ï¼Œä»£ç ç”Ÿæˆéœ€è¦ä¼˜åŒ–"
        config_type = "embed_only"
    elif chat_success:
        status = "ä»£ç ç”Ÿæˆæ­£å¸¸ï¼ŒåµŒå…¥éœ€è¦ä¼˜åŒ–"
        config_type = "chat_only"
    else:
        status = "éœ€è¦æ’æŸ¥é—®é¢˜"
        config_type = "needs_fix"
    
    print(f"\n æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   ğŸ”¤ åµŒå…¥åŠŸèƒ½: {'' if embed_success else ''}")
    print(f"   ğŸ’» ä»£ç ç”Ÿæˆ: {'' if chat_success else ''}")
    print(f"    çŠ¶æ€: {status}")
    
    # 7. ä¿å­˜é…ç½®
    if config_type != "needs_fix":
        save_optimized_config(base_url, code_model, embed_model, config_type)
        return True
    else:
        provide_troubleshooting_guide()
        return False

def save_optimized_config(base_url, code_model, embed_model, config_type):
    """ä¿å­˜ä¼˜åŒ–é…ç½®"""
    print(f"\n ä¿å­˜ä¼˜åŒ–é…ç½®...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    remote_config.set_remote_config(
        base_url=base_url,
        embedding_model=embed_model,
        code_model=code_model,
        fix_model=code_model
    )
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    config_content = f'''# ä¼˜åŒ–çš„è¿œç¨‹Ollamaé…ç½®
# é…ç½®ç±»å‹: {config_type}
# ç”Ÿæˆæ—¶é—´: {time.strftime("%Y-%m-%d %H:%M:%S")}

export OLLAMA_BASE_URL="{base_url}"
export OLLAMA_CODE_MODEL="{code_model}"
export OLLAMA_EMBEDDING_MODEL="{embed_model}"
export OLLAMA_FIX_MODEL="{code_model}"

# æ€§èƒ½ä¼˜åŒ–è®¾ç½®
export OLLAMA_REQUEST_TIMEOUT="120"  # è¯·æ±‚è¶…æ—¶æ—¶é—´
export OLLAMA_EMBED_TIMEOUT="60"     # åµŒå…¥è¶…æ—¶æ—¶é—´

echo " ä¼˜åŒ–çš„Ollamaç¯å¢ƒå·²è®¾ç½® (é…ç½®ç±»å‹: {config_type})"
echo "   æœåŠ¡å™¨: $OLLAMA_BASE_URL"
echo "   ä»£ç æ¨¡å‹: $OLLAMA_CODE_MODEL"
echo "   åµŒå…¥æ¨¡å‹: $OLLAMA_EMBEDDING_MODEL"

# é¢„çƒ­å»ºè®®
echo " é¦–æ¬¡ä½¿ç”¨å»ºè®®é¢„çƒ­æ¨¡å‹:"
echo "   curl -X POST \\$OLLAMA_BASE_URL/api/generate -d '{\\\"model\\\":\\\"{code_model}\\\",\\\"prompt\\\":\\\"hi\\\",\\\"stream\\\":false}'"
'''
    
    env_file = Path("optimized_ollama_env.sh")
    env_file.write_text(config_content)
    
    print(f" ä¼˜åŒ–é…ç½®å·²ä¿å­˜åˆ°: {env_file}")
    
    # æä¾›ä½¿ç”¨å»ºè®®
    print(f"\n ä½¿ç”¨å»ºè®®:")
    if config_type == "full":
        print(" æ‰€æœ‰åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹RAGå¼€å‘")
        print("1. source optimized_ollama_env.sh")
        print("2. python test_java_only.py")
    elif config_type == "embed_only":
        print(" ä»£ç ç”Ÿæˆæ¨¡å‹å“åº”æ…¢ï¼Œå»ºè®®:")
        print("1. åœ¨è¿œç¨‹æœåŠ¡å™¨é¢„çƒ­: ollama run qwen3-8b-q4:latest")
        print("2. source optimized_ollama_env.sh")
        print("3. python test_java_only.py")
    elif config_type == "chat_only":
        print(" åµŒå…¥æ¨¡å‹æœ‰é—®é¢˜ï¼Œå»ºè®®:")
        print("1. æ£€æŸ¥åµŒå…¥æ¨¡å‹: ollama run qwen3-embedding:latest")
        print("2. source optimized_ollama_env.sh")
        print("3. python test_java_only.py")

def provide_troubleshooting_guide():
    """æä¾›æ•…éšœæ’é™¤æŒ‡å—"""
    print(f"\n æ•…éšœæ’é™¤æŒ‡å—:")
    print("="*50)
    
    print("1. è¿œç¨‹æœåŠ¡å™¨æ“ä½œ:")
    print("   # æ£€æŸ¥OllamaçŠ¶æ€")
    print("   ps aux | grep ollama")
    print("   ")
    print("   # é‡å¯Ollama (å¦‚æœéœ€è¦)")
    print("   pkill ollama")
    print("   OLLAMA_HOST=0.0.0.0:11434 ollama serve")
    print("   ")
    print("   # é¢„çƒ­æ¨¡å‹")
    print("   ollama run qwen3-8b-q4:latest")
    print("   ollama run qwen3-embedding:latest")
    
    print("\n2. æ€§èƒ½ä¼˜åŒ–:")
    print("   # æ£€æŸ¥GPU/CPUä½¿ç”¨æƒ…å†µ")
    print("   nvidia-smi  # å¦‚æœä½¿ç”¨GPU")
    print("   htop        # æ£€æŸ¥CPUä½¿ç”¨")
    print("   ")
    print("   # è°ƒæ•´æ¨¡å‹å‚æ•° (åœ¨Modelfileä¸­)")
    print("   PARAMETER num_ctx 2048      # å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦")
    print("   PARAMETER num_predict 100   # é™åˆ¶è¾“å‡ºé•¿åº¦")
    
    print("\n3. ç½‘ç»œä¼˜åŒ–:")
    print("   # å¢åŠ è¶…æ—¶æ—¶é—´")
    print("   export OLLAMA_REQUEST_TIMEOUT=180")
    print("   ")
    print("   # æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ")
    print("   ping è¿œç¨‹æœåŠ¡å™¨IP")

def main():
    """ä¸»å‡½æ•°"""
    print(" è¿œç¨‹Ollamaä¼˜åŒ–é…ç½®åŠ©æ‰‹")
    print("ä¸“é—¨è§£å†³æ¨¡å‹å“åº”æ…¢å’Œè¶…æ—¶é—®é¢˜")
    print("="*60)
    
    if setup_optimized_remote():
        print(f"\n ä¼˜åŒ–é…ç½®å®Œæˆï¼")
        return True
    else:
        print(f"\n é…ç½®å¤±è´¥ï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤æŒ‡å—")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
